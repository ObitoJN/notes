# non-parameterized policy space
非参数化策略空间（non-parameterized policy space）是指在强化学习中，策略表示不依赖于固定参数向量的空间。传统的参数化策略通常使用固定长度的参数向量来表示策略函数，例如神经网络的权重向量。这些参数化策略具有固定的结构和参数数量。

相比之下，非参数化策略空间不依赖于固定参数向量，而是以其他方式表示策略。非参数化策略空间通常比参数化策略空间更灵活，可以适应更广泛的策略类别。它可以自由地扩展或缩小策略的规模和复杂度，而无需担心参数数量或结构的限制。

非参数化策略空间的一个例子是高斯过程策略（Gaussian Process Policy），它使用高斯过程来表示策略函数。高斯过程是一种概率模型，可以通过观测数据来学习策略函数的分布。在高斯过程策略中，策略的参数是无限维的，可以根据需要进行自由扩展。

非参数化策略空间的优势在于其灵活性和表达能力。然而，由于其无限维的特性，非参数化策略空间也可能面临计算上的挑战，例如需要处理高维数据或计算复杂的概率分布。因此，在实际应用中，选择参数化策略空间还是非参数化策略空间需要根据具体情况进行权衡。


# Wasserstein Distance
参考文章
> https://zhuanlan.zhihu.com/p/58506295

特点：
* 能够很自然地度量离散分布和连续分布之间的距离；
* 不仅给出了距离的度量，而且给出如何把一个分布变换为另一分布的方案；
* 能够连续地把一个分布变换为另一个分布，在此同时，能够保持分布自身的几何形态特征；

