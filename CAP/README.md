# Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning

## main contributions
* 可以保证训练过程每次迭代安全的可以感知不确定性的惩罚函数
* 可以动态调整训练过程中保守程度的自动更新规则
* 在简单的环境中（离散动作，奖励明确），可以找到零违反下的次优策略。
* 高维状态连续动作环境下，可扩展的CAP实现。
  
  主要解决了训练过程中的安全性。

## concept
### MBRL
MBRL代表模型基于增强学习（Model-Based Reinforcement Learning）。它是一种强化学习方法，结合了模型学习和策略优化的思想。

在传统的强化学习中，智能体（agent）通过与环境的交互来学习最优策略。然而，这种方法通常需要大量的试错，因为智能体必须不断尝试不同的行动并观察结果。而在MBRL中，智能体首先试图通过学习环境的模型来获得对环境的理解。这个模型可以是一个预测模型，能够预测在给定状态和动作下的下一个状态和奖励。

通过使用模型，智能体可以在内部进行模拟，而不必直接与环境交互。它可以使用模型来尝试各种可能的动作序列，并预测每个序列的结果。然后，智能体可以利用这些预测来选择最佳的动作序列，而无需实际进行试错。

一旦智能体利用模型生成了一系列动作，它就可以将这些动作序列应用于真实环境中进行验证。然后，智能体可以使用从实际环境中收集到的数据来更新和改进其模型，以便更准确地预测环境的动态。

MBRL的优点之一是可以利用模型学习的预测能力，来加速策略搜索和决策制定过程。它可以减少与环境的实际交互次数，从而在采样效率和学习速度上提供优势。然而，MBRL也面临着模型不准确性和不完备性的挑战，因为模型只是对环境的近似表示。因此，在实际应用中，需要平衡模型学习和策略优化的过程，并充分考虑模型的局限性。



  